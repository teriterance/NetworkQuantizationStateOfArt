@article{warren1,
    author  = "Warren S. McCulloch, Walter Pitts",
    title   = "Logical CAlculus Of the Ideas immanent in nervous Activity",
    year    = "1943",
    journal = "Bulletin of Mathematical Biology",
    volume  = "52",
    number  = "1/2",
    pages   = "99--115"
}

@article{RosenBlatt1,
  author          = {F. RosenBlatt},
  title           = {The perceptron: A probaabilistic model for information storage and organization in the brain},
  journal         = {Psychological Review},
  volume          = {65},
  number          = {6},
  year            = {1958}
}

@article{Rumelhart1,
  author          = {David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams},
  title           = {Learning representations by back-propagating errors},
  journal         = {Nature},
  volume          = {323},
  number          = {9},
  year            = {1986}
}

@article{Jurgen1,
  author          = {Jurgen Schmidhuber},
  title           = {Learning Complex, extended sequences using the principle of history compression},
  journal         = {Neural computation},
  volume          = {4},
  number          = {2},
  year            = {1992},
  pages           = {234-242}
}

@techreport{Ehud1,
  author      = {Ehud Y. Shapiro},
  title       = {inductive Inference of Theories From Facts},
  institution = {Yale University},
  year        = {1981}
}

@incollection{Muggleton1,
author = {STEPHEN MUGGLETON, WRAY BUNTINE},
title = {Machine Invention of First-order Predicates by Inverting Resolution},
editor = {John Laird},
booktitle = {Machine Learning Proceedings 1988},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {339-352},
year = {1988},
}

@article{Muggleton2,
  author          = {Stephen Muggleton},
  title           = {Inductive Logic Programming},
  journal         = {New Generation Computing},
  volume          = {8},
  pages           = {295-318},
  year            = {1991}
}

@techreport{Rajat1,
  author      = {Rajat Vikram Singh},
  title       = {ImageNet Winning CNN Architectures - A Review},
  institution = {andrew cmu},
  year        = {2016}
}

@article{MohamedIbn1,
  author          = {Mohamed Ibnkahla},
  title           = {Application of neural networks to digital communications - a survey},
  journal         = {Signal Processing},
  volume          = {80},
  pages           = {1185-1215},
  year            = {2000}
}

@article{XiaofanLi1,
  author          = {Xiaofan Li, Fangwei Dong, Sha Zhang},
  title           = {A Survey on Deep Learning Techniques in Wireless Signal Recognition},
  journal         = {Hindawi},
  volume          = {2019},
  number          = {12},
  year            = {2019}
}

@article{POZNYAK2019250,
title = {A survey on artificial neural networks application for identification and control in environmental engineering: Biological and chemical systems with uncertain models},
journal = {Annual Reviews in Control},
volume = {48},
pages = {250-272},
year = {2019},
author = {Alexander Poznyak and Isaac Chairez and Tatyana Poznyak},
}

@misc{jing2019survey,
      title={A Survey on Neural Network Language Models}, 
      author={Kun Jing and Jungang Xu},
      year={2019},
      eprint={1906.03591},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{MadhusmitaSahu,
author = {Sahu, Madhusmita and Dash, Rasmita},
journal = {Annual Reviews in Control},
year = {2021},
month = {01},
pages = {317-325},
title = {A Survey on Deep Learning: Convolution Neural Network (CNN)},
}

@article{Sornaminproceedings,
author = {Sornam, Madasamy and Kavitha, Muthu Subash and Venkateswaran, Vanitha},
year = {2017},
journal = {Annual Reviews in Control},
month = {12},
pages = {1},
title = {A Survey on Image Classification and Activity Recognition using Deep Convolutional Neural Network Architecture},
}

@article{Sussillo2014RandomWI,
  title={Random Walk Initialization for Training Very Deep Feedforward Networks},
  author={David Sussillo and L. Abbott},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2014}
}

@article{WU20093432,
title = {Global stability analysis of a general class of discontinuous neural networks with linear growth activation functions},
journal = {Information Sciences},
volume = {179},
number = {19},
pages = {3432-3441},
year = {2009},
author = {Huaiqin Wu},
}

@article{DeepBig,
author = {Cirean, Dan and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Jürgen},
year = {2012},
month = {01},
pages = {},
journal = {springer},
title = {Deep Big Multilayer Perceptrons For Digit Recognition},
}

@article{Canny1,
  author          = {John Canny},
  title           = {A Computational Approach to Edge Detection},
  journal         = {Transaction on pattern analysis and machine Intelligence},
  volume          = {8},
  number          = {6},
  year            = {1986}
}
@INPROCEEDINGS{8666928,
  author={W. {Wang} and J. {Gang}},
  booktitle={2018 International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
  title={Application of Convolutional Neural Network in Natural Language Processing}, 
  year={2018},
  volume={},
  number={},
  pages={64-70},
}

@misc{Browne1,
author = {Browne, Matthew and Ghidary, Saeed},
year = {2003},
month = {12},
pages = {641-652},
title = {Convolutional Neural Networks for Image Processing: An Application in Robot Vision},
isbn = {978-3-540-20646-0},
doi = {10.1007/978-3-540-24581-0_55}
}

@article{Gama_2019,
   title={Convolutional Neural Network Architectures for Signals Supported on Graphs},
   volume={67},
   ISSN={1941-0476},
   number={4},
   journal={IEEE Transactions on Signal Processing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Gama, Fernando and Marques, Antonio G. and Leus, Geert and Ribeiro, Alejandro},
   year={2019},
   month={Feb},
   pages={1034–1049}
}

@article{Hopfield,
  author          = {J.J Hopfield},
  title           = {Neural networks and physical systems with emergent collecive computational abilities},
  journal         = {Biophysics},
  volume          = {79},
  number          = {},
  year            = {1982}
}

@article{SLTMsurvey,
author = {Smagulova, Kamilya and James, Alex},
year = {2019},
month = {05},
pages = {},
title = {A survey on LSTM memristive neural network architectures and applications},
volume = {228},
journal = {The European Physical Journal Special Topics},
doi = {10.1140/epjst/e2019-900046-x}
}

@article{lstm1,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
}

@misc{cho2014learning,
      title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
      year={2014},
      eprint={1406.1078},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{heck2017simplified,
      title={Simplified Minimal Gated Unit Variations for Recurrent Neural Networks}, 
      author={Joel Heck and Fathi M. Salem},
      year={2017},
      eprint={1701.03452},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{zhou2016minimal,
      title={Minimal Gated Unit for Recurrent Neural Networks}, 
      author={Guo-Bing Zhou and Jianxin Wu and Chen-Lin Zhang and Zhi-Hua Zhou},
      year={2016},
      eprint={1603.09420},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@book{Rumelhart2,
author = {Rumelhart and E. David and Mcclelland James and L. James},
year = {1986},
month = {01},
publisher = {The MIT Press},
title = {Parallel distributed processing: explorations in the microstructure of cognition. Volume 1. Foundations}
}

@article{Salakhutdinov,
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
year = {2008},
month = {04},
journal = {MLR press},
title = {Learning and Evaluaing Deep Bolztmann Machines}
}